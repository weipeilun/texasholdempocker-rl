# texasholdempocker-rl

## 序言
先说一下这个项目的存在原因：
1. 人的生命有限，对自己不感兴趣的事，是绝无必要，也不应浪费生命的。
2. 有一次和朋友玩桌游的发现：对于同一个合作目标，大家的策略竟如此不同；虽然每个玩家的已知信息有差异，十几年的好兄弟就为个这玩意吵得就差动手了，也没办法说服对方。这实在是一件很爽的事。
3. 所以这能给我什么启发？我的人生，基本可以抽象成无限次的大规模协作场景的信息不对称的博弈游戏。[吴军博士](https://weibo.com/drwujun?sudaref=www.baidu.com&retcode=6102&is_hot=1&noscale_head=1)对人生做过类似的思考，真的很有趣的角度。所以我想独立思考一下，没准能发现点我自己的东西。这就是这个项目的根本初衷了。
4. 用什么场景来玩？信息不对称的无限博弈：牌类，桌游。有很深的策略空间：无限制下注德州扑克。
5. 开源出来的目的？我希望我原创的东西能给你提供点价值，鉴于我自己已经通过这个得到很多收获了，所以我想尽可能多的记录点过程。当然结果也很重要，毕竟这是给自己的东西，不是混职场，还是要show myself the code，不能说大话的。我还在迭代，更好的是能有什么反馈，所以欢迎提bug，或者任何东西。

## 策略选择

### 数学的最优解思路
这听起来很酷，人生维度上的纳什均衡，那不是无敌了。所以我读了一些[Noam Brown](https://noambrown.github.io/)的工作，他确实在限制下注的德州扑克中，从博弈论的角度严格证明的得到了数学最优解（纳什均衡），并且在可接受的时间内做了实现，《Science》封面大神。但是我仔细想了下，发现这不是我想要的。
1. 他的思想就是遍历：已知我的手牌，牌桌上所有其他的可能，包括公共牌、其他玩家的手牌、所有玩家在接下来的轮次中的所有可能行动，然后用了一个两阶段的算法做了极限优化。这是数学上的最优解，因为已经达到纳什均衡了，不会有比他给出的解达到更高的期望收益了，所以他产出的东西叫solver，问题解决了。
2. 一个题外话是，他的这种solver在牌桌上就非常好使了，只要轮数足够多，他不可能输。所以很多这种受他启发的solver在做着盈利的事。我对这种盈利是没兴趣的。
3. 从数学的角度，他给出的证明是非常牛逼的，但是确实太复杂了，也不是我想要的，我没时间研究。
4. 他的一个弊端，解决了有限注德州扑克问题，对于无限注德州扑克，搜索空间太大，他也没搞定了。
5. 嗯还有，我职业生涯早期在电商做过仓库的拣货路径优化问题，也跟这有类似的部分。那个用的是高度优化的A\*算法，本质也是遍历，只不过用了启发式搜索，然后分阶段计算，跟这个工作确实存在一些相同的思路，这就很有趣了。而且再想一下，A\*和Q-learning的遍历方式也是很像的，只不过Q-learning用的是优化的方法罢了。这样大规模启发式遍历算法，和强化学习的方法在最原始的最底层的思路上就关联上了，这是非常有趣的。
6. 到这就不禁想到OpenAI还没发布的Q\*了，是有一些神似的，而且Noam去OpenAI了。

### 强化学习的近似解思路
所以我想训出的是一个agent，像[AlphaGo](https://deepmind.google/discover/)，不求数学最优解，如果能接近人类的水平，就证明这个思路是有效的。在这个过程里，要克服的问题也是很多的。
1. 不用强化学习行不行？那就是监督学习。不是不可以，但是没啥意思。
2. 用传统的强化学习行不行？我确实在最初的版本里是用的DDPG，这和[AlphaGo论文](http://home.ustc.edu.cn/~ustcsh/py2016/data/nature16961.pdf)里的第二个步骤，用强化学习的方法去训练策略网络Pρ很像了。但是这有个问题： 在缺少第一步的监督学习的情况下（我穷），导致了跳过第一步直接进行第二步，网络训练的进程非常慢。而且实际跑起来这样就是无效的，因为这样生成的训练数据就是随机，唯一的信息是env给出的最终的结果的反馈，而整个状态空间又太大了，网络很容易在一个随机的初始值上就过拟合了。这个情况在[AlphaGo论文](http://home.ustc.edu.cn/~ustcsh/py2016/data/nature16961.pdf)里也有提到。
3. AlphaGo的方法到底行不行？我现在还不太确定，到现在的阶段主要考虑的是算力我是否能接受的问题。这个结论的出原因如下：
   1. 德州和棋类相比最大的不同是它是非完全信息博弈，就是其他玩家的手牌我是永远不知道的。这就带来了一个更复杂的估计的问题，估计的目标是在对手已知的历史行动下，我的预期收益（胜率）是多少，这意味着要通过对手的行为去预估对手隐藏的信息。而棋类游戏中没有这种隐式的信息，只根据所有显式的信息去预估就好了。因为预估胜率在MCTS中非常重要，MCTS的核心思想也就是“使用最大的算力集中计算最有价值的行动”，所以全部算法效果的计算核心都是围绕在收益（胜率）预估上了。
   2. 对于这个问题，第一个问题是这个胜率是否可以估计。因为对于不同的玩家性格，有的激进，有的保守，最重要的就是因为玩家在游戏过程中会有bluff的行为。这样对于不同玩家bluff的特征就是不一样的。我是没有可能刻画出这样的用户特征的。
   3. 但是从[AlphaGo论文](http://home.ustc.edu.cn/~ustcsh/py2016/data/nature16961.pdf)能得到一些启发，他在做强化学习的迭代时也遇到了相同的问题，也就是在最后一个模型估计v的时候（这同样也是关键的胜率预估步骤），选不同的模型得到的MCTS基线是不一样的，这就会导致完全不同的MCTS的结果。他最后也是为了计算量的考虑，特意单独训练了两个小模型，一个用于计算v，一个做fast rollout，在他的测评中这俩的效果比大模型直接做rollout都要差很多，但通过这两个模型产出的MCTS训出的大模型也超过了人类表现。所以不妨先做一个假设，只要固定下来一个基线模型，只要这个模型是有效的，那么它表现出的用户行为特征就是有一定的代表性的，用它来做胜率预估也就是能有一定的意义的。
   4. 这一点其实我认为是很关键的了。一个事实是很多量化的专业机构就会用AlphaGo的方法去做策略，去想为什么它的效果会如此之好？就是因为这是一个左右互搏的框架，这个框架刻画的就是股市中“非对称信息博弈”的场景啊！只不过股市中的玩家更多，变量更多而已了，但是这种数学表达的框架是很准确的了。所以要解决的核心问题也就是，在我已知的市场信息的条件下，要怎么估计我的每种操作的胜率的问题。我是无法预测在股市里实际操作的个人接下来会产生什么样的买卖行为的，但是如果放到群体中，就是能涌现出一定的统计层面的统计特征的，神经网络最擅长抓的就是这样的统计学特征。更重要的是再搭配上MCTS，它就可以超越监督学习，获得超过人类玩家的能力。
   5. 炒股这个场景我并不感兴趣。对于我来说的关键点是如何理解它能有这样的能力，这样的数学表达的原理是什么，以及到底是用什么样的步骤让它是有效的，我要把它真正的实现出来。这个工具的力量非常强大，这是我不断的加深感触的，但是这还不够，知道和做到永远是两码事。
4. 解决德扑的机器学习方法里还有一种叫DeepStack，这个我还没仔细思考。

## 特征、数据

## 模型、训练

## 工程优化
用了一些多进程和多线程的方法，实现一种可扩展的计算架构，基本复现了[AlphaGoZero论文](https://ics.uci.edu/~dechter/courses/ics-295/winter-2018/papers/nature-go.pdf)中的训练流程。